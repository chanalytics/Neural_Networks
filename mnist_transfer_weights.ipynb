{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Neural Networks\n",
    "Deep neural networks can take weeks to train on complex datasets even on fast GPU servers.  The mnist example we are using is simple so it is possible to train quickly on local machines.\n",
    "\n",
    "![file_name](https://cloud.githubusercontent.com/assets/17914936/20403127/2862931e-acc5-11e6-853c-02cac20c4ce1.png?style=centerme)\n",
    "\n",
    "Often in order to be useful on complex data you will need many more layers than we are using."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning\n",
    "\n",
    "One solution is to use transfer learning which is becomming more popular as people make their weights and network architecture available to others for use.\n",
    "\n",
    "![file_name](https://cloud.githubusercontent.com/assets/17914936/20403126/286226fe-acc5-11e6-9855-693183fab83e.png?style=centerme)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For transfer learning the network is doing forward propagation through all layers and then just updating weights through backward propagation in the unfrozen layers.  This is much much faster while still taking advantage of previous learning from the frozen layers. \n",
    "![file_name](https://github.com/JostineHo/neural-nets/blob/master/images/forward_back_prop.png?raw=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "# eveyone should stop and pip install h5py.  It is needed for weights loading.\n",
    "import h5py\n",
    "\n",
    "np.random.seed(1337)  # for reproducibility\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Convolution2D, MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras.models import model_from_json\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "nb_classes = 5\n",
    "nb_epoch = 5\n",
    "\n",
    "architecture_path = 'weights/final_arch.json'\n",
    "weights_path = 'weights/final_weights.h5'\n",
    "\n",
    "now = datetime.datetime.now\n",
    "\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 28, 28\n",
    "# number of convolutional filters to use\n",
    "nb_filters = 32\n",
    "# size of pooling area for max pooling\n",
    "pool_size = 2\n",
    "# convolution kernel size\n",
    "kernel_size = 3\n",
    "\n",
    "if K.image_dim_ordering() == 'th':\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    input_shape = (img_rows, img_cols, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we are loading the second half of the data our neural network hasn't seen yet.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the data, shuffled and split between train and test sets\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# create a dataset with digits including 5 and above\n",
    "X_train_gte5 = X_train[y_train >= 5]\n",
    "y_train_gte5 = y_train[y_train >= 5] - 5  # make classes start at 0 for\n",
    "X_test_gte5 = X_test[y_test >= 5]         # np_utils.to_categorical\n",
    "y_test_gte5 = y_test[y_test >= 5] - 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_model(model, train, test, nb_classes):\n",
    "    X_train = train[0].reshape((train[0].shape[0],) + input_shape)\n",
    "    X_test = test[0].reshape((test[0].shape[0],) + input_shape)\n",
    "    X_train = X_train.astype('float32')\n",
    "    X_test = X_test.astype('float32')\n",
    "    X_train /= 255\n",
    "    X_test /= 255\n",
    "    print('X_train shape:', X_train.shape)\n",
    "    print(X_train.shape[0], 'train samples')\n",
    "    print(X_test.shape[0], 'test samples')\n",
    "\n",
    "    # convert class vectors to binary class matrices\n",
    "    Y_train = np_utils.to_categorical(train[1], nb_classes)\n",
    "    Y_test = np_utils.to_categorical(test[1], nb_classes)\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adadelta',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    t = now()\n",
    "    model.fit(X_train, Y_train,\n",
    "              batch_size=batch_size, nb_epoch=nb_epoch,\n",
    "              verbose=1,\n",
    "              validation_data=(X_test, Y_test))\n",
    "    print('Training time: %s' % (now() - t))\n",
    "    score = model.evaluate(X_test, Y_test, verbose=0)\n",
    "    print('Test score:', score[0])\n",
    "    print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define two groups of layers: feature (convolutions) and classification (dense)\n",
    "feature_layers = [\n",
    "    Convolution2D(nb_filters, kernel_size, kernel_size,\n",
    "                  border_mode='valid',\n",
    "                  input_shape=input_shape),\n",
    "    Activation('relu'),\n",
    "    Convolution2D(nb_filters, kernel_size, kernel_size),\n",
    "    Activation('relu'),\n",
    "    MaxPooling2D(pool_size=(pool_size, pool_size)),\n",
    "    Dropout(0.25),\n",
    "    Flatten(),\n",
    "]\n",
    "classification_layers = [\n",
    "    Dense(128),\n",
    "    Activation('relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(nb_classes),\n",
    "    Activation('softmax')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create complete model\n",
    "model = Sequential(feature_layers + classification_layers)\n",
    "model.load_weights(weights_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# freeze feature layers and rebuild model\n",
    "for l in feature_layers:\n",
    "    l.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# transfer: train dense layers for new classification task [5..9]\n",
    "train_model(model,\n",
    "            (X_train_gte5, y_train_gte5),\n",
    "            (X_test_gte5, y_test_gte5), nb_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "All code was adapted from https://github.com/fchollet/keras/blob/master/examples/mnist_transfer_cnn.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
